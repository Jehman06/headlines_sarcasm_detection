{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc553ba0-521f-4013-9fb3-036e1cfb987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "with open(\"Sarcasm_Headlines_Dataset.json\", 'r') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)  # Parse each line as a JSON object\n",
    "        sentences.append(item['headline'])\n",
    "        labels.append(item['is_sarcastic'])\n",
    "        urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4b51855-a2cc-4dca-9282-31e9b49f2ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "(26709, 40)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7ae1107e-af49-4a0e-8218-c5247cd97de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: 21367\n",
      "Testing data shape: 5342\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training data shape:\", len(X_train))\n",
    "print(\"Testing data shape:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "855eede3-1d94-4aef-8ea7-59b7b9c2ba8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (21367, 40)\n",
      "Testing data shape: (5342, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Split the data first\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizer settings\n",
    "max_length = 40  # You can adjust this\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=100000, oov_token=oov_token)\n",
    "\n",
    "# Fit the tokenizer only on the training sentences\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert training and testing sentences to sequences\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to ensure consistent input length\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(\"Training data shape:\", training_padded.shape)\n",
    "print(\"Testing data shape:\", testing_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8ae0aefe-738a-4980-897e-c5b2afeec137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "embedding_dim = 32\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = 40\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.7),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59ec290e-6135-440b-a771-f0889d5a2861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "668/668 - 3s - loss: 0.6614 - accuracy: 0.5909 - val_loss: 0.5678 - val_accuracy: 0.7106 - lr: 0.0010 - 3s/epoch - 4ms/step\n",
      "Epoch 2/30\n",
      "668/668 - 3s - loss: 0.5215 - accuracy: 0.8003 - val_loss: 0.4682 - val_accuracy: 0.8347 - lr: 7.9433e-04 - 3s/epoch - 4ms/step\n",
      "Epoch 3/30\n",
      "668/668 - 3s - loss: 0.4471 - accuracy: 0.8643 - val_loss: 0.4293 - val_accuracy: 0.8557 - lr: 6.3096e-04 - 3s/epoch - 4ms/step\n",
      "Epoch 4/30\n",
      "668/668 - 3s - loss: 0.4025 - accuracy: 0.8867 - val_loss: 0.4098 - val_accuracy: 0.8574 - lr: 5.0119e-04 - 3s/epoch - 4ms/step\n",
      "Epoch 5/30\n",
      "668/668 - 3s - loss: 0.3730 - accuracy: 0.9018 - val_loss: 0.3971 - val_accuracy: 0.8613 - lr: 3.9811e-04 - 3s/epoch - 4ms/step\n",
      "Epoch 6/30\n",
      "668/668 - 3s - loss: 0.3491 - accuracy: 0.9086 - val_loss: 0.3928 - val_accuracy: 0.8611 - lr: 3.1623e-04 - 3s/epoch - 4ms/step\n",
      "Epoch 7/30\n",
      "668/668 - 3s - loss: 0.3291 - accuracy: 0.9183 - val_loss: 0.3895 - val_accuracy: 0.8624 - lr: 2.5119e-04 - 3s/epoch - 4ms/step\n",
      "Epoch 8/30\n",
      "668/668 - 3s - loss: 0.3190 - accuracy: 0.9205 - val_loss: 0.3863 - val_accuracy: 0.8617 - lr: 1.9953e-04 - 3s/epoch - 4ms/step\n",
      "Epoch 9/30\n",
      "668/668 - 3s - loss: 0.3113 - accuracy: 0.9193 - val_loss: 0.3862 - val_accuracy: 0.8611 - lr: 1.5849e-04 - 3s/epoch - 4ms/step\n",
      "Epoch 10/30\n",
      "668/668 - 3s - loss: 0.3056 - accuracy: 0.9260 - val_loss: 0.3881 - val_accuracy: 0.8596 - lr: 1.2589e-04 - 3s/epoch - 4ms/step\n",
      "Epoch 11/30\n",
      "668/668 - 3s - loss: 0.2991 - accuracy: 0.9251 - val_loss: 0.3860 - val_accuracy: 0.8592 - lr: 1.0000e-04 - 3s/epoch - 4ms/step\n",
      "Epoch 12/30\n",
      "668/668 - 2s - loss: 0.2928 - accuracy: 0.9265 - val_loss: 0.3816 - val_accuracy: 0.8618 - lr: 7.9433e-05 - 2s/epoch - 3ms/step\n",
      "Epoch 13/30\n",
      "668/668 - 2s - loss: 0.2904 - accuracy: 0.9285 - val_loss: 0.3841 - val_accuracy: 0.8589 - lr: 6.3096e-05 - 2s/epoch - 4ms/step\n",
      "Epoch 14/30\n",
      "668/668 - 2s - loss: 0.2891 - accuracy: 0.9281 - val_loss: 0.3844 - val_accuracy: 0.8590 - lr: 5.0119e-05 - 2s/epoch - 3ms/step\n",
      "Epoch 15/30\n",
      "668/668 - 3s - loss: 0.2858 - accuracy: 0.9298 - val_loss: 0.3856 - val_accuracy: 0.8581 - lr: 3.9811e-05 - 3s/epoch - 5ms/step\n",
      "Epoch 16/30\n",
      "668/668 - 3s - loss: 0.2821 - accuracy: 0.9309 - val_loss: 0.3841 - val_accuracy: 0.8592 - lr: 3.1623e-05 - 3s/epoch - 4ms/step\n",
      "Epoch 17/30\n",
      "668/668 - 5s - loss: 0.2821 - accuracy: 0.9291 - val_loss: 0.3840 - val_accuracy: 0.8594 - lr: 2.5119e-05 - 5s/epoch - 8ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "training_labels = np.array(training_labels)\n",
    "testing_labels = np.array(testing_labels)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10 ** (-(epoch / 10)))\n",
    "\n",
    "# Now fit the model\n",
    "history = model.fit(\n",
    "    training_padded, \n",
    "    training_labels, \n",
    "    epochs=num_epochs, \n",
    "    validation_data=(\n",
    "        testing_padded, \n",
    "        testing_labels\n",
    "    ), \n",
    "    verbose=2, \n",
    "    callbacks=[\n",
    "        early_stopping, \n",
    "        lr_schedule,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "49d22a13-4343-429a-affc-d452a0e1608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "[[0.64829034]\n",
      " [0.08248807]]\n",
      "Sentence: 'Bruce Springsteen Songs Ranked by the Degree of Flagrancy with Which He Uses the Word “Daddy”' is Sarcastic\n",
      "Sentence: 'Harris tries to turn the tables on Trump by calling him 'unhinged'' is Not Sarcastic\n"
     ]
    }
   ],
   "source": [
    "sentence = [\n",
    "    \"Bruce Springsteen Songs Ranked by the Degree of Flagrancy with Which He Uses the Word “Daddy”\",\n",
    "    \"Harris tries to turn the tables on Trump by calling him 'unhinged'\",\n",
    "]\n",
    "\n",
    "# Tokenize and pad the sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Get the predicted probabilities\n",
    "predictions = model.predict(padded)\n",
    "print(predictions)\n",
    "\n",
    "# Convert the probabilities to \"Sarcastic\" or \"Not Sarcastic\"\n",
    "threshold = 0.5\n",
    "for i, prediction in enumerate(predictions):\n",
    "    if prediction > threshold:\n",
    "        print(f\"Sentence: '{sentence[i]}' is Sarcastic\")\n",
    "    else:\n",
    "        print(f\"Sentence: '{sentence[i]}' is Not Sarcastic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6989edb8-22d9-484a-9dcf-47d160b14580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
